{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a demo of using Gradual Magnitude Pruning to sparsify Pointcloud object detection algorithm (pointnet++)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network and run GMP using sparseML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import importlib\n",
    "import shutil\n",
    "from models.pointnet2_utils import PointNetSetAbstraction\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, Callable\n",
    "import pandas as pd\n",
    "from data_utils.ModelNetDataLoader import ModelNetDataLoader\n",
    "import provider\n",
    "\n",
    "\n",
    "import sparseml\n",
    "from sparseml.pytorch.optim import ScheduledModifierManager\n",
    "from sparseml.pytorch.utils import TensorBoardLogger, ModuleExporter, get_prunable_layers, tensor_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PointNet++ Model\n",
    "class get_model(nn.Module):\n",
    "    def __init__(self,num_class,normal_channel=True):\n",
    "        super(get_model, self).__init__()\n",
    "        in_channel = 6 if normal_channel else 3\n",
    "        self.normal_channel = normal_channel\n",
    "        self.sa1 = PointNetSetAbstraction(npoint=512, radius=0.2, nsample=32, in_channel=in_channel, mlp=[64, 64, 128], group_all=False)\n",
    "        self.sa2 = PointNetSetAbstraction(npoint=128, radius=0.4, nsample=64, in_channel=128 + 3, mlp=[128, 128, 256], group_all=False)\n",
    "        self.sa3 = PointNetSetAbstraction(npoint=None, radius=None, nsample=None, in_channel=256 + 3, mlp=[256, 512, 1024], group_all=True)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.drop1 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.fc3 = nn.Linear(256, num_class)\n",
    "\n",
    "    def forward(self, xyz):\n",
    "        B, _, _ = xyz.shape\n",
    "        if self.normal_channel:\n",
    "            norm = xyz[:, 3:, :]\n",
    "            xyz = xyz[:, :3, :]\n",
    "        else:\n",
    "            norm = None\n",
    "        l1_xyz, l1_points = self.sa1(xyz, norm)\n",
    "        l2_xyz, l2_points = self.sa2(l1_xyz, l1_points)\n",
    "        l3_xyz, l3_points = self.sa3(l2_xyz, l2_points)\n",
    "        x = l3_points.view(B, 1024)\n",
    "        x = self.drop1(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.drop2(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, -1)\n",
    "\n",
    "\n",
    "        return x, l3_points\n",
    "\n",
    "class get_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(get_loss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, trans_feat):\n",
    "        total_loss = F.nll_loss(pred, target)\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and load pretrained model\n",
    "model = get_model(10, False)\n",
    "model = model.cuda()\n",
    "checkpoint = torch.load('checkpoints/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Save the dense model for future inference\n",
    "dense_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa1.mlp_convs.0\n",
      "sa1.mlp_convs.1\n",
      "sa1.mlp_convs.2\n",
      "sa2.mlp_convs.0\n",
      "sa2.mlp_convs.1\n",
      "sa2.mlp_convs.2\n",
      "sa3.mlp_convs.0\n",
      "sa3.mlp_convs.1\n",
      "sa3.mlp_convs.2\n",
      "fc1\n",
      "fc2\n",
      "fc3\n"
     ]
    }
   ],
   "source": [
    "# Print the prunable layers using sparseml\n",
    "for (name, layer) in get_prunable_layers(model):\n",
    "    print(f\"{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_recipe_path = \"pointnet_recipy.yaml\"\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train data is 3991\n",
      "The size of test data is 908\n"
     ]
    }
   ],
   "source": [
    "#load the data\n",
    "data_path = 'data/modelnet40_normal_resampled/'\n",
    "args = argparse.Namespace(batch_size=24, decay_rate=0.0001, epoch=200, gpu='0', learning_rate=0.001, log_dir='pointnet2_cls_ssg', model='pointnet2_cls_ssg', num_category=10, num_point=1024, optimizer='Adam', process_data=False, use_cpu=False, use_normals=False, use_uniform_sample=False)\n",
    "train_dataset = ModelNetDataLoader(root=data_path, args=args, split='train', process_data=args.process_data)\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=args.process_data)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 20:48:08 sparseml.pytorch.utils.logger INFO     Logging all SparseML modifier-level logs to sparse_logs/08-05-2023_20.48.08.log\n"
     ]
    }
   ],
   "source": [
    "# create ScheduledModifierManager and Optimizer wrapper\n",
    "manager = ScheduledModifierManager.from_yaml(pruning_recipe_path)\n",
    "logger = TensorBoardLogger(log_path=\"./tensorboard_outputs\")\n",
    "optimizer = manager.modify(model, optimizer, loggers=[logger], steps_per_epoch=len(trainDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Write wrapper for running one epoch for test or train\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "def run_model_one_epoch(model, data_loader, criterion, device, train=False, optimizer=None):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # loop through batches\n",
    "    mean_correct = []\n",
    "    for batch_id, (points, target) in tqdm(enumerate(data_loader, 0), total=len(data_loader), smoothing=0.9):\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        points = points.data.numpy()\n",
    "        # print(\"Before processing\",points.shape)\n",
    "        points = provider.random_point_dropout(points)\n",
    "        points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n",
    "        points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n",
    "        points = torch.Tensor(points)\n",
    "        points = points.transpose(2, 1)\n",
    "\n",
    "        # print(\"After processing\",points.shape)\n",
    "        if not args.use_cpu:\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "\n",
    "        pred, trans_feat = model(points)\n",
    "        loss = criterion(pred, target.long(), trans_feat)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # global_step += 1\n",
    "\n",
    "    instance_acc = np.mean(mean_correct)\n",
    "    # print('Instance Accuracy: %f' % instance_acc)\n",
    "\n",
    "    return instance_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Training Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [00:38<00:00,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1/15\n",
      "Training Acc: 0.9656124497991967\n",
      "\n",
      "Validation Epoch: 1/15\n",
      "Validation Acc: 0.9232456140350876\n",
      "\n",
      "Running Training Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:36<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 2/15\n",
      "Training Acc: 0.9678714859437753\n",
      "\n",
      "Validation Epoch: 2/15\n",
      "Validation Acc: 0.9320175438596491\n",
      "\n",
      "Running Training Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 3/15\n",
      "Training Acc: 0.968624497991968\n",
      "\n",
      "Validation Epoch: 3/15\n",
      "Validation Acc: 0.9375\n",
      "\n",
      "Running Training Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 4/15\n",
      "Training Acc: 0.9683734939759037\n",
      "\n",
      "Validation Epoch: 4/15\n",
      "Validation Acc: 0.9364035087719299\n",
      "\n",
      "Running Training Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:38<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 5/15\n",
      "Training Acc: 0.96285140562249\n",
      "\n",
      "Validation Epoch: 5/15\n",
      "Validation Acc: 0.9298245614035089\n",
      "\n",
      "Running Training Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 6/15\n",
      "Training Acc: 0.963855421686747\n",
      "\n",
      "Validation Epoch: 6/15\n",
      "Validation Acc: 0.9396929824561404\n",
      "\n",
      "Running Training Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:37<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 7/15\n",
      "Training Acc: 0.958835341365462\n",
      "\n",
      "Validation Epoch: 7/15\n",
      "Validation Acc: 0.9320175438596493\n",
      "\n",
      "Running Training Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:38<00:00,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 8/15\n",
      "Training Acc: 0.962098393574297\n",
      "\n",
      "Validation Epoch: 8/15\n",
      "Validation Acc: 0.9188596491228072\n",
      "\n",
      "Running Training Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 9/15\n",
      "Training Acc: 0.9585843373493974\n",
      "\n",
      "Validation Epoch: 9/15\n",
      "Validation Acc: 0.930921052631579\n",
      "\n",
      "Running Training Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:37<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 10/15\n",
      "Training Acc: 0.9515562248995982\n",
      "\n",
      "Validation Epoch: 10/15\n",
      "Validation Acc: 0.9287280701754387\n",
      "\n",
      "Running Training Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 11/15\n",
      "Training Acc: 0.9585843373493976\n",
      "\n",
      "Validation Epoch: 11/15\n",
      "Validation Acc: 0.9111842105263158\n",
      "\n",
      "Running Training Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 12/15\n",
      "Training Acc: 0.9613453815261043\n",
      "\n",
      "Validation Epoch: 12/15\n",
      "Validation Acc: 0.9122807017543857\n",
      "\n",
      "Running Training Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 13/15\n",
      "Training Acc: 0.9578313253012049\n",
      "\n",
      "Validation Epoch: 13/15\n",
      "Validation Acc: 0.9232456140350876\n",
      "\n",
      "Running Training Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:35<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 14/15\n",
      "Training Acc: 0.9560742971887549\n",
      "\n",
      "Validation Epoch: 14/15\n",
      "Validation Acc: 0.9155701754385964\n",
      "\n",
      "Running Training Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 166/166 [00:39<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Validation Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:08<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 15/15\n",
      "Training Acc: 0.9595883534136548\n",
      "\n",
      "Validation Epoch: 15/15\n",
      "Validation Acc: 0.9287280701754387\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run GMP algorithm\n",
    "epoch = 0\n",
    "for epoch in range(manager.max_epochs):\n",
    "    # run training loop\n",
    "    epoch_name = f\"{epoch + 1}/{manager.max_epochs}\"\n",
    "    print(f\"Running Training Epoch {epoch_name}\")\n",
    "    train_acc = run_model_one_epoch(model, trainDataLoader, criterion, device, train=True, optimizer=optimizer)\n",
    "    # print(f\"Training Epoch: {epoch_name}\\nTraining Loss: {train_loss}\\nTop 1 Acc: {train_acc}\\n\")\n",
    "\n",
    "    # run validation loop\n",
    "    print(f\"Running Validation Epoch {epoch_name}\")\n",
    "    val_acc = run_model_one_epoch(model, testDataLoader, criterion, device, train=False, optimizer=optimizer)\n",
    "    # print(f\"Validation Epoch: {epoch_name}\\nVal Loss: {val_loss}\\nTop 1 Acc: {val_acc}\\n\")\n",
    "    \n",
    "    # logger.log_scalar(\"Metrics/Loss (Train)\", train_loss, epoch)\n",
    "    print(f\"Training Epoch: {epoch_name}\\nTraining Acc: {train_acc}\\n\")\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Train)\", train_acc, epoch)\n",
    "    # logger.log_scalar(\"Metrics/Loss (Validation)\", val_loss, epoch)\n",
    "    print(f\"Validation Epoch: {epoch_name}\\nValidation Acc: {val_acc}\\n\")\n",
    "    logger.log_scalar(\"Metrics/Accuracy (Validation)\", val_acc, epoch)\n",
    "\n",
    "    \n",
    "manager.finalize(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate different performance and sparsity metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code for running inference\n",
    "def test(model, loader, num_class=40, vote_num=1):\n",
    "    mean_correct = []\n",
    "    classifier = model.eval()\n",
    "    class_acc = np.zeros((num_class, 3))\n",
    "\n",
    "    for j, (points, target) in tqdm(enumerate(loader), total=len(loader)):\n",
    "        if not args.use_cpu:\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "\n",
    "        points = points.transpose(2, 1)\n",
    "        vote_pool = torch.zeros(target.size()[0], num_class).cuda()\n",
    "\n",
    "        for _ in range(vote_num):\n",
    "            pred, _ = classifier(points)\n",
    "            vote_pool += pred\n",
    "        pred = vote_pool / vote_num\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "        for cat in np.unique(target.cpu()):\n",
    "            classacc = pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum()\n",
    "            class_acc[cat, 0] += classacc.item() / float(points[target == cat].size()[0])\n",
    "            class_acc[cat, 1] += 1\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "\n",
    "    class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n",
    "    class_acc = np.mean(class_acc[:, 2])\n",
    "    instance_acc = np.mean(mean_correct)\n",
    "    return instance_acc, class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:09<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense Instance Test Accuracy 0.9451754385964913\n",
      "Dense Class Test Accuracy 0.9473511904761904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 38/38 [00:09<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Instance Test Accuracy 0.9473684210526315\n",
      "Sparse Class Test Accuracy 0.9502678571428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dense Model Accuracy\n",
    "\n",
    "dense_instance_acc, dense_class_acc = test(dense_model,testDataLoader,num_class=10)\n",
    "print(\"Dense Instance Test Accuracy\",dense_instance_acc)\n",
    "print(\"Dense Class Test Accuracy\",dense_class_acc)\n",
    "\n",
    "# Sparse Model Accuracy\n",
    "sparse_model = model\n",
    "sparse_instance_acc, sparse_class_acc = test(sparse_model,testDataLoader,num_class=10)\n",
    "print(\"Sparse Instance Test Accuracy\",sparse_instance_acc)\n",
    "print(\"Sparse Class Test Accuracy\",sparse_class_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sparse model\n",
    "\n",
    "save_dir = \"checkpoints/\"\n",
    "exporter = ModuleExporter(sparse_model, output_dir=save_dir)\n",
    "exporter.export_pytorch(name=\"sparse_best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa1.mlp_convs.0.weight: 0.2135\n",
      "sa1.mlp_convs.1.weight: 0.4832\n",
      "sa1.mlp_convs.2.weight: 0.5154\n",
      "sa2.mlp_convs.0.weight: 0.6508\n",
      "sa2.mlp_convs.1.weight: 0.6293\n",
      "sa2.mlp_convs.2.weight: 0.6190\n",
      "sa3.mlp_convs.0.weight: 0.8001\n",
      "sa3.mlp_convs.1.weight: 0.8503\n",
      "sa3.mlp_convs.2.weight: 0.9526\n",
      "fc1.weight: 0.9407\n",
      "fc2.weight: 0.8120\n",
      "fc3.weight: 0.3227\n"
     ]
    }
   ],
   "source": [
    "# Get weight sparsity\n",
    "\n",
    "wsp = []\n",
    "names = []\n",
    "for (name, layer) in get_prunable_layers(model):\n",
    "    names.append(name)\n",
    "    wsp.append(tensor_sparsity(layer.weight).item())\n",
    "    print(f\"{name}.weight: {tensor_sparsity(layer.weight).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa1.mlp_convs.0 (Conv2d) - Input sparsity: 0.8501, Output sparsity: 0.4730\n",
      "sa1.mlp_convs.1 (Conv2d) - Input sparsity: 0.4588, Output sparsity: 0.8510\n",
      "sa1.mlp_convs.2 (Conv2d) - Input sparsity: 0.1592, Output sparsity: 0.7684\n",
      "sa2.mlp_convs.0 (Conv2d) - Input sparsity: 0.5714, Output sparsity: 0.2998\n",
      "sa2.mlp_convs.1 (Conv2d) - Input sparsity: 0.4123, Output sparsity: 0.5830\n",
      "sa2.mlp_convs.2 (Conv2d) - Input sparsity: 0.5363, Output sparsity: 0.8186\n",
      "sa3.mlp_convs.0 (Conv2d) - Input sparsity: 0.5933, Output sparsity: 0.4405\n",
      "sa3.mlp_convs.1 (Conv2d) - Input sparsity: 0.6598, Output sparsity: 0.7056\n",
      "sa3.mlp_convs.2 (Conv2d) - Input sparsity: 0.6446, Output sparsity: 0.4845\n",
      "fc1 (Linear) - Input sparsity: 0.0682, Output sparsity: 0.5190\n",
      "fc2 (Linear) - Input sparsity: 0.5338, Output sparsity: 0.4832\n",
      "fc3 (Linear) - Input sparsity: 0.4870, Output sparsity: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# Get input and activation sparsity\n",
    "\n",
    "insp = []\n",
    "osp = []\n",
    "\n",
    "def remove_all_forward_hooks(model: torch.nn.Module) -> None:\n",
    "    for name, child in model._modules.items():\n",
    "        if child is not None:\n",
    "            if hasattr(child, \"_forward_hooks\"):\n",
    "                child._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "            remove_all_forward_hooks(child)\n",
    "remove_all_forward_hooks(model)\n",
    "\n",
    "def sparsity_hook(layer_name, layer):\n",
    "    def hook(_, input, output):\n",
    "        # input_sparsity = (input[0] == 0).sum().item() / input[0].numel()\n",
    "        # print(input[0].shape)\n",
    "        input_sparsity = tensor_sparsity(input[0]).item()\n",
    "        insp.append(input_sparsity)\n",
    "        # print\n",
    "        # output_sparsity = ((torch.abs(output) > 0.1) == 0).sum().item() / output.numel()\n",
    "        output_sparsity = (torch.relu(output) == 0).sum().item() / output.numel()\n",
    "        osp.append(output_sparsity)\n",
    "        print(f\"{layer_name} ({layer.__class__.__name__}) - Input sparsity: {input_sparsity:.4f}, Output sparsity: {output_sparsity:.4f}\")\n",
    "    return hook\n",
    "\n",
    "def register_hooks(module, module_name):\n",
    "    if isinstance(module, (nn.Conv2d, nn.ReLU, nn.Linear)):\n",
    "        hook = sparsity_hook(module_name, module)\n",
    "        module.register_forward_hook(hook)\n",
    "    # else:\n",
    "    #     print(f\"Skipping {module_name} ({module.__class__.__name__})\")\n",
    "\n",
    "\n",
    "# Register hooks for each module (including submodules)\n",
    "for name, layer in model.named_modules():\n",
    "    register_hooks(layer, name)\n",
    "\n",
    "data = torch.randn(24, 1024, 3)\n",
    "\n",
    "data = data.transpose(2, 1)\n",
    "data = data.to(device)\n",
    "\n",
    "\n",
    "# run model on random data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(data)\n",
    "    # print(output.shape)\n",
    "    # print(f\"Output sparsity: {tensor_sparsity(output).item():.4f}\")\n",
    "\n",
    "# print(insp)\n",
    "# print(osp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 8)\n"
     ]
    }
   ],
   "source": [
    "# Make a dataframe and store the sparsities\n",
    "df = pd.DataFrame(list(zip(names, wsp, insp, osp)), columns =['Layer', 'Weight Sparsity', 'Input Sparsity', 'Output Sparsity'])\n",
    "\n",
    "# compute the densities for each layer which is the inverse of the sparsity\n",
    "df['Weight Density'] = 1 - df['Weight Sparsity']\n",
    "df['Input Density'] = 1 - df['Input Sparsity']\n",
    "df['Output Density'] = 1 - df['Output Sparsity']\n",
    "\n",
    "# put layer number as column\n",
    "df['Layer Number'] = df.index + 1\n",
    "\n",
    "df.to_csv(\"Layer-wise-sparsities.csv\",index=False)\n",
    "reordered_columns = ['Layer',  'Layer Number', 'Input Density', 'Weight Density', 'Output Density', 'Weight Sparsity', 'Input Sparsity', 'Output Sparsity']\n",
    "\n",
    "df2 = df[reordered_columns]\n",
    "print(df2.shape)\n",
    "df2.to_csv(\"Layer-wise-sparsities-2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparsePoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42dce22a43a30ef55b5a96b37e043450a4cefd06f0baa4f0f94ca51cbda64cf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
